{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based image retrieval\n",
    "\n",
    "**Content-based image retrieval** (CBIR) is the mechanism of retrieving images relevant to a given query from a large collection of images known as an image database, based on their semantic or visual content rather than on derived attributes or keyword descriptors prescriptively defined for them.\n",
    "\n",
    "As shown in the picture there are some basic steps involved in query and retrieval:\n",
    "\n",
    "- Feature extraction: Of course, this part involves the extraction of image characteristics, such as texture, color, etc. It could also be considered a preprocessing step to, for example, resize or improve the quality of the images.\n",
    "- Similarity measure: The similarity measurement is used to estimate the query image with the database images by similarity. The dissimilarity between the feature vector of the query image and the database images is calculated using different distance metrics. The higher the dissimilarity, the less similar the two images are. Some commonly used distances are: Euclidean distance, block distance, Minkowski distance and Mahalanobis distance.\n",
    "- Retrieve the results: The N most similar images are displayed to the user.\n",
    "\n",
    "![CBIR](https://raw.githubusercontent.com/upm-classes/image-understanding-2021-2022/main/practice2/figures/cbir_01.jpg \"CBIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels of the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {0:'airplane', \n",
    "          1: 'automobile',\n",
    "          2: 'bird',\n",
    "          3: 'cat',\n",
    "          4: 'deer',\n",
    "          5: 'dog',\n",
    "          6: 'frog',\n",
    "          7: 'horse',\n",
    "          8: 'ship',\n",
    "          9: 'truck'\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and rescale to a [0, 1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "print(f'shape train set: {x_train.shape}, shape test set: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the first 10 images of the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES = 10\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "for i in range(NUM_IMAGES):\n",
    "    im = x_train[i, :, :, :]\n",
    "    ax = fig.add_subplot(2, 5, i + 1)\n",
    "    ax.imshow(im)\n",
    "    ax.set_title(LABELS[i])\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction phase with an autoencoder\n",
    "\n",
    "An autoencoder is a neural network that is unsupervised which means that doesn't require any labeled data. \n",
    "\n",
    "They work by compressing the input into a latent space representation and reconstructing the output from this representation:\n",
    "\n",
    "- Encoder: the part of the network that compresses the input into a latent space  representation (i.e., representation of compressed data). It can be represented by an encoding function \\\\( h=f(x) \\\\).\n",
    "- Decoder: This part aims to reconstruct the input from the latent space representation. It can be represented by a decoding function \\\\( r=g(h) \\\\).\n",
    "\n",
    "![CBIR Autoencoder](https://raw.githubusercontent.com/upm-classes/image-understanding-2021-2022/main/practice2/figures/cbir_02.jpg \"CBIR Autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, num_filters=32, latent_dim=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(num_filters, kernel_size=3, \n",
    "                                   activation=\"relu\", strides=2, padding='same', name='enc_conv1')\n",
    "        self.conv2 = layers.Conv2D(num_filters * 2, kernel_size=3, \n",
    "                                   activation=\"relu\", padding='same', name='enc_conv2')\n",
    "        self.conv3 = layers.Conv2D(num_filters * 2, kernel_size=3, \n",
    "                                   activation=\"relu\", padding='same', strides=2, name='enc_conv3')\n",
    "        self.conv4 = layers.Conv2D(num_filters * 2, kernel_size=3, \n",
    "                                   activation=\"relu\", padding='same', name='enc_conv4')\n",
    "        self.conv5 = layers.Conv2D(num_filters * 2, kernel_size=3, \n",
    "                                   activation=\"relu\", padding='same', strides=2, name='enc_conv5')\n",
    "        self.flatten = layers.Flatten(name='enc_flatten')\n",
    "        self.dense = layers.Dense(latent_dim, activation=\"relu\", name='enc_dense')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder(num_filters=32, latent_dim=128, name='encoder')\n",
    "encoder.build((1, 32, 32, 3))\n",
    "encoder.summary()\n",
    "\n",
    "class Decoder(keras.Model):\n",
    "    def __init__(self, num_filters=32, output_channels=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense = layers.Dense(16 * num_filters, activation=\"relu\", name='dec_dense')\n",
    "        self.reshape = layers.Reshape((4, 4, num_filters), name='dec_reshape')\n",
    "        self.upsam1 = layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\", name='dec_upsampling1')\n",
    "        self.conv1 = layers.Conv2D(num_filters * 4, kernel_size=3, activation=\"relu\", \n",
    "                                   padding='same', name='dec_conv1')\n",
    "        self.upsam2 = layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\", name='dec_upsampling2')\n",
    "        self.conv2 = layers.Conv2D(num_filters * 2, kernel_size=3, activation=\"relu\", \n",
    "                                   padding='same', name='dec_conv2')\n",
    "        self.upsam3 = layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\", name='dec_upsampling3')\n",
    "        self.conv3 = layers.Conv2D(output_channels, kernel_size=3, activation=\"sigmoid\", \n",
    "                                   padding='same', name='dec_conv3')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.reshape(x)\n",
    "        x = self.upsam1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.upsam2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.upsam3(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "decoder = Decoder(num_filters=32, output_channels=3, name='decoder')\n",
    "decoder.build((1, 128))\n",
    "decoder.summary()\n",
    "\n",
    "class AutoEncoder(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "autoencoder = AutoEncoder(encoder, decoder, name='autoencoder') \n",
    "optimizer = Adam()\n",
    "autoencoder.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mse\"])\n",
    "autoencoder.build((1, 32, 32, 3))\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained model\n",
    "! wget https://github.com/upm-classes/image-understanding-2021-2022/blob/main/practice2/autoencoder_cifar10_2.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'autoencoder_cifar10_2.h5'\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    autoencoder.fit(x_train, x_train,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            validation_data=(x_test, x_test))\n",
    "    autoencoder.save_weights(filename)\n",
    "else:\n",
    "    autoencoder.load_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test how the autoencoder works with N random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 5\n",
    "idx_random = np.random.choice(x_train.shape[0], num_images)\n",
    "\n",
    "y_hat = autoencoder.predict(x_train[idx_random])\n",
    "\n",
    "fig = plt.figure(figsize=(4, num_images*3))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "for i in range(num_images):\n",
    "    im = x_train[idx_random, :, :, :]\n",
    "    ax1 = fig.add_subplot(6, 2, i * 2 + 1)\n",
    "    ax1.imshow(x_train[idx_random[i]])\n",
    "    ax1.set_title(LABELS[y_train[idx_random[i]][0]])\n",
    "    ax1.axis('off')\n",
    "    ax2 = fig.add_subplot(6, 2, i * 2 + 2)\n",
    "    ax2.imshow(y_hat[i])\n",
    "    ax2.set_title(LABELS[y_train[idx_random[i]][0]])\n",
    "    ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity calculation and retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves calculating the similarity between all the images of the database and a test image.\n",
    "\n",
    "As you can imagine this is high-demanding process (calculating all the distances), so there are different techniques that can be used to streamlite this process, such as precalculating the distances, use of distances trees.\n",
    "\n",
    "As our database is not big, we are going to use for calculating the distance an algorithm called \"k-nearest neighbors\" (k-NN). k-NN is a non-parametric supervised learning method used for classification that determines the class/label of a new sample by taking into account the k closest training examples in a dataset. When k is set to one (k = 1), then input sample is assigned to the single nearest neighbor.\n",
    "\n",
    "In our case, we are going to use the distances calculated for k-NN algorithm to retrieve the N most similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes some time to calculate the features for the train and testing sets\n",
    "\n",
    "def extract_features(images:np.array) -> np.array:\n",
    "    # Here we use the encoder to return the latent features of the input images\n",
    "    feats = encoder.predict(images)\n",
    "    return feats\n",
    "\n",
    "train_feats = extract_features(x_train) # Train set\n",
    "test_feats = extract_features(x_test) # Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our similarity measure, the euclidean distance between x and y. \n",
    "# The closer to 0 the more similar x and y are.\n",
    "\n",
    "def euclidean_distance(x: np.array, y: np.array) -> float:\n",
    "    distance = np.sum((x-y)**2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set IDX_SAMPLE to the number of your team\n",
    "\n",
    "IDX_SAMPLE = 1000 # This corresponds to the index of the query image\n",
    "NUM_SAMPLES = 9 # Number of images to retrieve\n",
    "\n",
    "query_feats = test_feats[IDX_SAMPLE].reshape(1, -1)    \n",
    "\n",
    "# This function retreive the indexes and distances of the n neighbors most similar to the query image\n",
    "knn = KNeighborsClassifier(n_neighbors=1, metric=euclidean_distance).fit(train_feats, y_train.ravel())\n",
    "retrieved_distances, retrieved_idx = knn.kneighbors(query_feats, n_neighbors=NUM_SAMPLES, return_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the images\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "ax = fig.add_subplot(2, 5, 1)\n",
    "\n",
    "ax.imshow(x_test[IDX_SAMPLE,:,:,:])\n",
    "ax.set_title(f'Test image: {LABELS[y_test[IDX_SAMPLE][0]]}')\n",
    "ax.axis('off')\n",
    "for i in range(NUM_SAMPLES):\n",
    "    im = x_train[np.squeeze(retrieved_idx)[i], :, :, :]\n",
    "    ax = fig.add_subplot(2, 5, i + 2)\n",
    "    ax.imshow(im)\n",
    "    ax.set_title(LABELS[y_train[np.squeeze(retrieved_idx)[i]][0]])\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: implement a distance based on cosine similarity. \n",
    "# Hint: closer images to the sample should have a smaller distance\n",
    "\n",
    "def cosine_distance(x:np.array, y:np.array) -> float:\n",
    "    distance = None # TODO\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, metric=cosine_distance).fit(train_feats, y_train.ravel())\n",
    "\n",
    "query_feats = test_feats[IDX_SAMPLE].reshape(1, -1)  \n",
    "\n",
    "# This function retreive the indexes and distances of the n neighbors most similar to the query image\n",
    "retrieved_distances, retrieved_idx = knn.kneighbors(query_feats, n_neighbors=NUM_SAMPLES, \n",
    "                                                    return_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Show the distances associated with each retrieved image as part of the subtitle (only 2 decimal places)\n",
    "\n",
    "def visualize_similar_images(retrieved_idx:np.array, retrieved_distances:np.array) -> None:\n",
    "    fig = plt.figure(figsize=(16, 4))\n",
    "    fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    ax = fig.add_subplot(2, 5, 1)\n",
    "\n",
    "    ax.imshow(x_test[IDX_SAMPLE,:,:,:])\n",
    "    ax.set_title(f'Test image: {LABELS[y_test[IDX_SAMPLE][0]]}')\n",
    "    ax.axis('off')\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        im = x_train[np.squeeze(retrieved_idx)[i], :, :, :]\n",
    "        ax = fig.add_subplot(2, 5, i + 2)\n",
    "        ax.imshow(im)\n",
    "        ax.set_title(LABELS[y_train[np.squeeze(retrieved_idx)[i]][0]])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "visualize_similar_images(retrieved_idx, retrieved_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using raw pixels as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the function to use the raw pixels as features\n",
    "\n",
    "def extract_features_raw(images:np.array) -> np.array:\n",
    "    feats = None # TODO. Hint the shape of the output should be: [N, 32x32x3]\n",
    "    return feats\n",
    "\n",
    "train_feats_raw = extract_features_raw(x_train) # Train set\n",
    "test_feats_raw = extract_features_raw(x_test) # Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the results with raw pixels using the euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, metric=euclidean_distance).fit(train_feats_raw, y_train.ravel())\n",
    "\n",
    "query_feats_raw = test_feats_raw[IDX_SAMPLE].reshape(1, -1)    \n",
    "\n",
    "# This function retreive the indexes and distances of the n neighbors most similar to the query image\n",
    "retrieved_distances, retrieved_idx = knn.kneighbors(query_feats_raw, n_neighbors=NUM_SAMPLES, \n",
    "                                                    return_distance=True)\n",
    "\n",
    "visualize_similar_images(retrieved_idx, retrieved_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the results with raw pixels using the cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, metric=cosine_distance).fit(train_feats_raw, y_train.ravel())\n",
    "\n",
    "query_feats_raw = test_feats_raw[IDX_SAMPLE].reshape(1, -1)    \n",
    "\n",
    "# This function retreive the indexes and distances of the n neighbors most similar to the query image\n",
    "retrieved_distances, retrieved_idx = knn.kneighbors(query_feats_raw, n_neighbors=NUM_SAMPLES, \n",
    "                                                    return_distance=True)\n",
    "\n",
    "visualize_similar_images(retrieved_idx, retrieved_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using histogram as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the function to use the histogram as features\n",
    "\n",
    "def extract_features_hist(images:np.array) -> np.array:\n",
    "    feats = None # TODO. Hint the shape of the output should be: [N, 256x3]\n",
    "    return feats\n",
    "\n",
    "train_feats_hist = extract_features_hist(x_train) # Train set\n",
    "test_feats_hist = extract_features_hist(x_test) # Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the results with raw pixels using the euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, metric=euclidean_distance).fit(train_feats_hist, y_train.ravel())\n",
    "\n",
    "query_feats_hist = test_feats_hist[IDX_SAMPLE].reshape(1, -1)    \n",
    "\n",
    "# This function retreive the indexes and distances of the n neighbors most similar to the query image\n",
    "retrieved_distances, retrieved_idx = knn.kneighbors(query_feats_hist, n_neighbors=NUM_SAMPLES, \n",
    "                                                    return_distance=True)\n",
    "\n",
    "visualize_similar_images(retrieved_idx, retrieved_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, metric=cosine_distance).fit(train_feats_hist, y_train.ravel())\n",
    "\n",
    "query_feats_hist = test_feats_hist[IDX_SAMPLE].reshape(1, -1)    \n",
    "\n",
    "# This function retreive the indexes and distances of the n neighbors most similar to the query image\n",
    "retrieved_distances, retrieved_idx = knn.kneighbors(query_feats_hist, n_neighbors=NUM_SAMPLES, \n",
    "                                                    return_distance=True)\n",
    "\n",
    "visualize_similar_images(retrieved_idx, retrieved_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
